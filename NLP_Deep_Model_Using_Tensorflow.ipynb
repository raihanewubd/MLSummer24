{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raihanewubd/MLSummer24/blob/main/NLP_Deep_Model_Using_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "t_qywUkS0L2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data source: https://data.mendeley.com/datasets/8fbdhh72gs/5"
      ],
      "metadata": {
        "id": "ypHJ5K7c0Pgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "\n",
        "!pip install -q transformers==4.30.0\n",
        "!pip install -q  tensorflow==2.12.0"
      ],
      "metadata": {
        "id": "L3vXNO4dzlsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "id": "xwTZ3AKc0DUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset\n"
      ],
      "metadata": {
        "id": "eFTj7vtsnwps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usro1BKGmpU5"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1VEanmIImFMEkqt_4S9RuEdpUqQYMZaro"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/balanced_emotion_subset.csv')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LOsAdEtBn8wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text']"
      ],
      "metadata": {
        "id": "g5iryXTRuLDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "0i1ArUl1oGUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Word cloud for text data\n",
        "text = ' '.join(df['full_text'])\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hVuh5HQnqxAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the value counts of the 'emotion' column\n",
        "emotion_counts = df['emotion'].value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=emotion_counts.index, y=emotion_counts.values)\n",
        "plt.title('Distribution of Emotions')\n",
        "plt.xlabel('Emotion')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)  # Rotate labels if necessary\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gUyNCZbzrRWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Data Analysis"
      ],
      "metadata": {
        "id": "BdCUG6xhrZZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_and_tokenize(text):\n",
        "    # Remove special characters and numbers (optional, based on your requirement)\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    # Tokenize by splitting the text into words\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('/content/balanced_emotion_subset.csv')\n",
        "\n",
        "# Apply tokenization to your text column (replace 'text_column' with your actual text column name)\n",
        "data['tokens'] = data['full_text'].apply(clean_and_tokenize)\n",
        "\n",
        "# Flatten the list of tokens and count occurrences\n",
        "all_tokens = [token for sublist in data['tokens'] for token in sublist]\n",
        "token_counts = Counter(all_tokens)\n",
        "\n",
        "# Display the most common tokens\n",
        "most_common_tokens = token_counts.most_common(20)  # Adjust the number as needed\n",
        "print(most_common_tokens)\n"
      ],
      "metadata": {
        "id": "_GR8fWWWrn1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate tokens and their counts for plotting\n",
        "tokens, counts = zip(*most_common_tokens)\n",
        "\n",
        "# Creating a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(tokens, counts)\n",
        "plt.title('Top 10 Most Common Tokens')\n",
        "plt.xlabel('Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CL7W22AGrzr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using dataframe data: plot emotion bar plot\n",
        "\n",
        "#data.plot(kind='bar', x='emotion', y='value')\n",
        "data['emotion'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "RY-IVMPRvCbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/balanced_emotion_subset.csv')\n",
        "\n",
        "# Stop words list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean, tokenize and remove stop words\n",
        "def process_text(text):\n",
        "    # Lowercase and remove special characters\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text).lower()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Apply text processing\n",
        "data['processed_tokens'] = data['clean_text'].apply(process_text)\n",
        "\n",
        "# Flatten the list of tokens and count occurrences\n",
        "all_tokens = [token for sublist in data['processed_tokens'] for token in sublist]\n",
        "token_counts = Counter(all_tokens)\n",
        "\n",
        "# Generate bigrams and trigrams\n",
        "bigrams = [bigram for sublist in data['processed_tokens'] for bigram in ngrams(sublist, 2)]\n",
        "trigrams = [trigram for sublist in data['processed_tokens'] for trigram in ngrams(sublist, 3)]\n",
        "\n",
        "# Count bigrams and trigrams\n",
        "bigram_counts = Counter(bigrams)\n",
        "trigram_counts = Counter(trigrams)\n",
        "\n",
        "# Function to plot most common n-grams\n",
        "def plot_ngrams(ngram_counts, title, n=10):\n",
        "    # Get most common n-grams\n",
        "    most_common_ngrams = ngram_counts.most_common(n)\n",
        "    # Prepare data for plotting\n",
        "    labels = [' '.join(ngram) for ngram, count in most_common_ngrams]\n",
        "    values = [count for ngram, count in most_common_ngrams]\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(labels, values)\n",
        "    plt.title(f'Top {n} Most Common {title}')\n",
        "    plt.xlabel(title)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Plotting\n",
        "plot_ngrams(token_counts, 'Words')\n",
        "plot_ngrams(bigram_counts, 'Bigrams')\n",
        "plot_ngrams(trigram_counts, 'Trigrams')\n"
      ],
      "metadata": {
        "id": "3bGCrrAxsYEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train BERT Model"
      ],
      "metadata": {
        "id": "R_n0scDyqk6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) is a groundbreaking model in the field of natural language processing (NLP) developed by Google. Its structure and approach represented a significant shift in how language models were traditionally built. Here's an overview of its structure:\n",
        "\n",
        "* Transformer Architecture: BERT is based on the Transformer architecture, which was introduced in the paper \"Attention is All You Need\" by Vaswani et al. The Transformer model is primarily known for its use of self-attention mechanisms, which enable the model to weigh the importance of different words in a sentence.\n",
        "\n",
        "* Bidirectional Context: Unlike previous models that processed text in a single direction (either left-to-right or right-to-left), BERT reads the entire sequence of words at once. This bidirectionality allows the model to understand the context of a word based on all of its surroundings (both left and right of the word).\n",
        "\n",
        "* Layers in BERT:\n",
        "\n",
        "> * Input Embedding Layer: This layer converts each input token (word or subword) into vectors that represent them. These embeddings include token embeddings, segment embeddings, and position embeddings.\n",
        "> > * Transformer Blocks: These are the core of the BERT model. Each block contains two sub-layers: a multi-head self-attention mechanism and a fully connected feed-forward network. BERT models vary in size, but the BERT-Base model contains 12 of these blocks, while BERT-Large has 24.\n",
        "> > * Normalization and Activation Functions: Within each Transformer block, there are normalization steps and activation functions (like GELU - Gaussian Error Linear Units) that aid in the training and effectiveness of the model.\n",
        "Pre-training and Fine-tuning:\n",
        "\n",
        "> * Pre-training: BERT is pre-trained on a large corpus of text in an unsupervised manner using two tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). In MLM, some percentage of the input tokens are masked, and the model is trained to predict them. In NSP, the model learns to predict if one sentence logically follows another.\n",
        "> * Fine-tuning: After pre-training, BERT can be fine-tuned with additional output layers for various specific tasks like question answering, sentiment analysis, etc. During fine-tuning, the model is trained on a smaller, task-specific dataset.\n",
        "> * Output: The output of BERT can be tailored for various NLP tasks. For instance, in classification tasks, the output from the [CLS] token (a special token added at the beginning of each input) is used for the final classification.\n",
        "\n",
        "BERT's ability to understand the context of words in a sentence from both directions and its versatility in adapting to different NLP tasks made it a significant milestone in the field of AI and language understanding."
      ],
      "metadata": {
        "id": "xjUfcFkPIySU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for local systems\n",
        "# in colab its already installed\n",
        "#pip install transfermers"
      ],
      "metadata": {
        "id": "sevtGUJ_xoqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "\n",
        "\n",
        "texts = df['full_text']  # or 'clean_text' based on your choice\n",
        "labels = df['emotion']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text into input IDs, attention masks, and token type IDs\n",
        "def tokenize(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "train_encodings = tokenize(train_texts.tolist())\n",
        "test_encodings = tokenize(test_texts.tolist())\n",
        "\n",
        "# Convert labels to numeric values\n",
        "label_to_id = {label: id for id, label in enumerate(df['emotion'].unique())}\n",
        "train_labels = train_labels.apply(lambda x: label_to_id[x])\n",
        "test_labels = test_labels.apply(lambda x: label_to_id[x])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HMpES05cn254"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "\n",
        "# Disable distribution strategy if not needed\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "texts = df['full_text']  # or 'clean_text' based on your choice\n",
        "labels = df['emotion']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text into input IDs, attention masks, and token type IDs\n",
        "def tokenize(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "train_encodings = tokenize(train_texts.tolist())\n",
        "test_encodings = tokenize(test_texts.tolist())\n",
        "\n",
        "# Convert labels to numeric values\n",
        "label_to_id = {label: id for id, label in enumerate(df['emotion'].unique())}\n",
        "train_labels = train_labels.apply(lambda x: label_to_id[x])\n",
        "test_labels = test_labels.apply(lambda x: label_to_id[x])\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['emotion'].unique()))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=5e-5)\n",
        "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
        "    train_labels,\n",
        "    batch_size=8,\n",
        "    epochs=3,\n",
        "    validation_data=({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}, test_labels)\n",
        ")\n"
      ],
      "metadata": {
        "id": "TrWJVReKuFze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['emotion'].unique()))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=5e-5)\n",
        "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "# Train the model\n",
        "\"\"\"\n",
        "model.fit(\n",
        "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
        "    train_labels,\n",
        "    batch_size=8,\n",
        "    epochs=3,\n",
        "    validation_data=({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}, test_labels)\n",
        ")\"\"\"\n",
        "\n",
        "history = model.fit(\n",
        "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
        "    train_labels,\n",
        "    batch_size=8,\n",
        "    epochs=3,\n",
        "    validation_data=({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}, test_labels)\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "#model.evaluate({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']}, test_labels)"
      ],
      "metadata": {
        "id": "Mw6MrsHCoM6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pJRdqTAjob7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.predict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']})\n",
        "predicted_labels = tf.argmax(predictions.logits, axis=1)\n"
      ],
      "metadata": {
        "id": "mNTE3GpFo_SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Convert predictions and labels to numpy arrays for metric calculation\n",
        "predicted_labels = predicted_labels.numpy()\n",
        "true_labels = test_labels.to_numpy()\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "class_report = classification_report(true_labels, predicted_labels, target_names=label_to_id.keys(), output_dict=True)\n"
      ],
      "metadata": {
        "id": "-NtGU9xOpQ6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g', xticklabels=label_to_id.keys(), yticklabels=label_to_id.keys())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print precision, recall, and F1-score\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels, target_names=label_to_id.keys()))\n"
      ],
      "metadata": {
        "id": "mpjFsxrdpSy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ELMO model"
      ],
      "metadata": {
        "id": "C0mIf0mt1RuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ELMO (Embeddings from Language Models) model, introduced by researchers from the Allen Institute for AI, is a significant advancement in the field of natural language processing (NLP). It's designed to understand the context and meaning of words in a sentence more effectively than traditional word embeddings like Word2Vec or GloVe. Here's an overview of its structure and features:\n",
        "\n",
        "* Contextualized Word Embeddings: Unlike traditional word embeddings, ELMO generates embeddings that are context-dependent. This means that the representation for a word can change based on the sentence in which it appears, allowing ELMO to capture nuances in meaning and syntax.\n",
        "\n",
        "* Deep Bidirectional Language Model: ELMO uses a deep, bidirectional Long Short-Term Memory (LSTM) network. The bidirectional aspect means that it processes each word by looking at the context from both the left and the right side of the word in a sentence. This approach is different from earlier unidirectional language models, which only considered context from one direction (either left-to-right or right-to-left).\n",
        "\n",
        "* Layered Representation: ELMO's model extracts features from multiple layers of the LSTM network. In a typical ELMO model, there are two LSTM layers stacked on top of each other. Each layer captures different types of information; for example, lower layers might capture syntactic aspects while higher layers capture more of the semantic content.\n",
        "\n",
        "* Pre-training on Large Text Corpus: Before being used for specific tasks, ELMO is pre-trained on a large text corpus. During this phase, it learns a broad understanding of language, including grammar, word usage, and common phrases.\n",
        "\n",
        "* Task-Specific Fine-Tuning: After pre-training, ELMO embeddings can be fine-tuned for specific tasks, like sentiment analysis, question answering, or language translation. This fine-tuning allows ELMO to adapt its general language understanding to the nuances of the specific task.\n",
        "\n",
        "* Output and Integration: The final output of ELMO is a set of vectors for each word in the input sentence. These vectors can be easily integrated into various NLP models (like neural networks for classification tasks) to enhance their performance by providing more accurate and context-aware word representations."
      ],
      "metadata": {
        "id": "LpJlzrLmIS3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tensorflow tensorflow-hub pandas scikit-learn"
      ],
      "metadata": {
        "id": "QPdOmpg5JtHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "VfX9vUocJo4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "file_path = '/content/balanced_emotion_subset.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocess data\n",
        "texts = data['clean_text'].astype(str)\n",
        "labels = pd.factorize(data['sentiment_type'])[0]\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "RlC1WhFr2oG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
        "\n",
        "def elmo_embeddings(texts):\n",
        "    embeddings = elmo.signatures['default'](tf.constant(texts))['elmo']\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "R8Xw3uZY2qdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_generator(X, y, batch_size):\n",
        "    while True:\n",
        "        for offset in range(0, len(X), batch_size):\n",
        "            batch_texts = X[offset:offset+batch_size]\n",
        "            batch_labels = y[offset:offset+batch_size]\n",
        "            yield elmo_embeddings(batch_texts.tolist()), batch_labels\n",
        "\n",
        "def validation_batch_generator(X, y, batch_size):\n",
        "    while True:\n",
        "        for offset in range(0, len(X), batch_size):\n",
        "            batch_texts = X[offset:offset + batch_size]\n",
        "            batch_labels = y[offset:offset + batch_size]\n",
        "            yield elmo_embeddings(batch_texts.tolist()), batch_labels\n"
      ],
      "metadata": {
        "id": "P-gAEpEONrcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1024, 1024), dtype=tf.float32),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "bXKYbi23RN3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "steps_per_epoch = max(1, len(X_train) // batch_size)\n",
        "validation_steps = max(1, len(X_test) // batch_size)\n",
        "\n",
        "history = model.fit(batch_generator(X_train, y_train, batch_size),\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_data=validation_batch_generator(X_test, y_test, batch_size),\n",
        "                    validation_steps=validation_steps,\n",
        "                    epochs=10)\n"
      ],
      "metadata": {
        "id": "mdiAz3rARP52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_qTSJXR9Rq7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert probabilities to discrete predictions\n",
        "test_pred_labels = np.argmax(test_pred, axis=1)\n",
        "\n",
        "# Calculate metrics with appropriate averaging for multi-class classification\n",
        "precision = precision_score(y_test, test_pred_labels, average='macro')\n",
        "recall = recall_score(y_test, test_pred_labels, average='macro')\n",
        "f1 = f1_score(y_test, test_pred_labels, average='macro')\n",
        "conf_matrix = confusion_matrix(y_test, test_pred_labels)\n",
        "\n",
        "# Display metrics\n",
        "print(f\"Precision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ],
      "metadata": {
        "id": "3KPodO_nTYy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XLNet"
      ],
      "metadata": {
        "id": "TV_bOmUko25H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/balanced_emotion_subset.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe to understand its structure\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "FESOWkHu0wA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install SentencePiece\n",
        "# then restart the session"
      ],
      "metadata": {
        "id": "rFyEFE5A03yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "DVNY9ICY3IXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from transformers import TFRobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "#data = pd.read_csv('path_to_your_dataset.csv')\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['encoded_emotion'] = label_encoder.fit_transform(data['emotion'])\n",
        "\n",
        "# Ensure the text column is a list of strings\n",
        "data['clean_text'] = data['clean_text'].astype(str)\n",
        "\n",
        "# Split the dataset\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(\n",
        "    data['clean_text'].tolist(), data['encoded_emotion'].tolist(), test_size=0.3, random_state=42\n",
        ")\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(\n",
        "    temp_text, temp_labels, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "def tokenize_function(text_list):\n",
        "    return tokenizer(text_list, padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Tokenize data\n",
        "train_encodings = tokenize_function(train_text)\n",
        "val_encodings = tokenize_function(val_text)\n",
        "test_encodings = tokenize_function(test_text)\n",
        "\n",
        "# Convert labels to TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))\n",
        "\n",
        "# Model\n",
        "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16, validation_data=val_dataset.batch(16))\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = model.evaluate(test_dataset.batch(16))\n",
        "print(f\"Test Loss: {eval_results[0]}, Test Accuracy: {eval_results[1]}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Ww5SOMs52pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions for confusion matrix\n",
        "y_pred = model.predict(test_dataset.batch(16)).logits\n",
        "y_pred = tf.argmax(y_pred, axis=1).numpy()\n",
        "\n",
        "# Get the unique classes in test_labels\n",
        "unique_classes = np.unique(test_labels)\n",
        "\n",
        "# Generate target names based on unique classes\n",
        "target_names = [label_encoder.inverse_transform([cls])[0] for cls in unique_classes]\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(test_labels, y_pred))\n"
      ],
      "metadata": {
        "id": "zoNT1H8M8jUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jJojWX83Juol"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}